{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-29T17:03:59.113632Z",
     "start_time": "2025-03-29T17:03:59.107653Z"
    }
   },
   "source": [
    "NUM_QUERIES = 1000\n",
    "RESULTS_LEN = 400\n",
    "SLATE_LENGTH = 400\n",
    "NUM_FEATURES = 300\n",
    "\n",
    "RUN_ID = 1"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T17:04:22.354563Z",
     "start_time": "2025-03-29T17:03:59.117259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data/data_export.pkl\", \"rb\") as file:\n",
    "    property_map, grouped_rankings = pickle.load(file)"
   ],
   "id": "c081ad6c286c789d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T17:04:23.881994Z",
     "start_time": "2025-03-29T17:04:22.438484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.transforms import transforms\n",
    "from allrank.data.dataset_loading import LibSVMDataset, FixLength, ToTensor, fix_length_to_longest_slate\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "def flatten_groups(grouped_rankings):\n",
    "    ranking_groups = []\n",
    "\n",
    "    for search_date, search_date_grouped_items in grouped_rankings.items():\n",
    "        for _, check_in_grouped_items in search_date_grouped_items.items():\n",
    "            for _, duration_grouped_items in check_in_grouped_items.items():\n",
    "                for _, guest_count_grouped_items in duration_grouped_items.items():\n",
    "                    ranking_groups.append(guest_count_grouped_items[:])\n",
    "\n",
    "    return ranking_groups\n",
    "\n",
    "\n",
    "def combine_data(ranking_groups, property_map, slate_length=400, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    total_rows = 0\n",
    "    group_sizes = []\n",
    "    \n",
    "    for idx, ranking_group in enumerate(ranking_groups):\n",
    "        group_rankings = [item[\"ranking\"] for item in ranking_group]\n",
    "        group_rankings_np = np.array(group_rankings)\n",
    "        \n",
    "        _, unique_indices = np.unique(group_rankings_np[::-1], return_index=True)\n",
    "        unique_count = len(unique_indices)\n",
    "        \n",
    "        group_sizes.append(unique_count)\n",
    "        total_rows += unique_count\n",
    "    \n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    \n",
    "    # Get feature dimension from first item in first group to allocate array\n",
    "    sample_item = ranking_groups[0][0]\n",
    "    ranking_attrs = sample_item[\"scaled_attrs\"]\n",
    "    property_id = sample_item[\"propertyId\"]\n",
    "    prop = property_map[property_id]\n",
    "    property_data = np.concatenate([prop[\"attrs\"], prop[\"encoded_title\"]])\n",
    "    full_attrs = np.concatenate([ranking_attrs, property_data])\n",
    "    feature_dim = len(full_attrs)\n",
    "    \n",
    "    print(f\"Feature dimension: {feature_dim}\")\n",
    "    \n",
    "    # Pre-allocate arrays - now including the attributes array\n",
    "    all_attrs = np.zeros((total_rows, feature_dim), dtype=np.float32)\n",
    "    all_rankings = np.zeros(total_rows, dtype=np.float32)\n",
    "    all_queries = np.zeros(total_rows, dtype=np.int32)\n",
    "    \n",
    "    query_index = {}\n",
    "    current_row = 0\n",
    "    \n",
    "    # Second pass - fill arrays\n",
    "    for idx, ranking_group in enumerate(ranking_groups):\n",
    "        group_data = []\n",
    "        group_rankings = []\n",
    "        listing_ids = []\n",
    "        group_metadata = {}\n",
    "\n",
    "        for item in ranking_group:\n",
    "            ranking_attrs = item[\"scaled_attrs\"]\n",
    "            property_id = item[\"propertyId\"]\n",
    "            ranking = item[\"ranking\"]\n",
    "\n",
    "            prop = property_map[property_id]\n",
    "            property_data = np.concatenate([prop[\"attrs\"], prop[\"encoded_title\"]])\n",
    "            full_attrs = np.concatenate([ranking_attrs, property_data])\n",
    "            \n",
    "            group_data.append(full_attrs)\n",
    "            group_rankings.append(ranking)\n",
    "            listing_ids.append({\n",
    "                'airbnb_id': prop[\"airbnbId\"],\n",
    "                'guesty_listing_id': prop[\"guestyListingId\"]\n",
    "            })\n",
    "\n",
    "            group_metadata = {\n",
    "                \"durationOfStay\": item[\"durationOfStay\"],\n",
    "                \"searchDateOffset\": item[\"searchDateOffset\"],\n",
    "                \"checkInDateOffset\": item[\"checkInDateOffset\"],\n",
    "                \"guestCount\": item[\"guestCount\"]\n",
    "            }\n",
    "        \n",
    "        group_rankings_np = np.array(group_rankings)\n",
    "        group_data_np = np.array(group_data)\n",
    "\n",
    "        # Remove duplicates\n",
    "        _, unique_indices = np.unique(group_rankings_np[::-1], return_index=True)\n",
    "        unique_indices = unique_indices[::-1]\n",
    "\n",
    "        group_data_np = group_data_np[unique_indices]\n",
    "        group_rankings_np = group_rankings_np[unique_indices]\n",
    "        listing_ids = [listing_ids[i] for i in unique_indices]\n",
    "\n",
    "        # Shuffle\n",
    "        perm = np.random.permutation(len(group_data_np))\n",
    "        group_data_np = group_data_np[perm]\n",
    "        group_rankings_np = group_rankings_np[perm]\n",
    "        listing_ids = [listing_ids[i] for i in perm]\n",
    "\n",
    "        # Invert ranking\n",
    "        inverted_rankings = np.clip(slate_length - group_rankings_np, 0, None)\n",
    "        \n",
    "        # Store all data directly in pre-allocated arrays\n",
    "        group_size = len(group_data_np)\n",
    "        all_attrs[current_row:current_row+group_size] = group_data_np\n",
    "        all_rankings[current_row:current_row+group_size] = inverted_rankings\n",
    "        all_queries[current_row:current_row+group_size] = idx\n",
    "        \n",
    "        # Update query index\n",
    "        query_index[idx] = {\n",
    "            **group_metadata,\n",
    "            'listingIds': listing_ids\n",
    "        }\n",
    "        \n",
    "        current_row += group_size\n",
    "        \n",
    "    return all_attrs, all_rankings, all_queries, query_index\n",
    "\n",
    "def get_dataset(ds_attrs, ds_ranks, ds_qids, is_train, slate_length):\n",
    "    attrs_sparse = csr_matrix(ds_attrs)\n",
    "    ds = LibSVMDataset(attrs_sparse, ds_ranks, ds_qids)\n",
    "    \n",
    "    if is_train:\n",
    "        ds.transform = transforms.Compose([FixLength(slate_length), ToTensor()])\n",
    "    else:\n",
    "        ds.transform = fix_length_to_longest_slate(ds)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def get_datasets(grouped_rankings, property_map, valid_percent, slate_length, seed=42):\n",
    "    ranking_groups = flatten_groups(grouped_rankings)\n",
    "    total_groups = len(ranking_groups)\n",
    "\n",
    "    # Shuffle group order\n",
    "    indices = np.random.permutation(total_groups)\n",
    "    valid_size = int(valid_percent * total_groups)\n",
    "\n",
    "    valid_groups = [ranking_groups[i] for i in indices[:valid_size]]\n",
    "    train_groups = [ranking_groups[i] for i in indices[valid_size:]]\n",
    "\n",
    "    # Combine groups into arrays\n",
    "    train_attrs, train_ranks, train_qids, train_index = combine_data(train_groups, property_map, slate_length, seed)\n",
    "    valid_attrs, valid_ranks, valid_qids, valid_index = combine_data(valid_groups, property_map, slate_length, seed)\n",
    "    \n",
    "    train_ds = get_dataset(train_attrs, train_ranks, train_qids, True, slate_length)\n",
    "    valid_ds = get_dataset(valid_attrs, valid_ranks, valid_qids, False, slate_length)\n",
    "\n",
    "    return train_ds, valid_ds, train_index, valid_index"
   ],
   "id": "6d4b6ef66e8f94cd",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T17:11:32.597465Z",
     "start_time": "2025-03-29T17:04:23.891704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_ds, valid_ds, train_index, valid_index = get_datasets(\n",
    "    grouped_rankings, \n",
    "    property_map, \n",
    "    0.3, \n",
    "    SLATE_LENGTH)"
   ],
   "id": "dbf1fc8d83683def",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 3155204\n",
      "Feature dimension: 3180\n",
      "Total rows: 1361651\n",
      "Feature dimension: 3180\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T17:11:33.202832Z",
     "start_time": "2025-03-29T17:11:33.074302Z"
    }
   },
   "cell_type": "code",
   "source": "train_ds[0]",
   "id": "3258408f312eeee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000,  0.2000,  0.3119,  ...,  0.0384,  0.0290,  0.0493],\n",
       "         [ 0.0000,  0.2000,  0.3119,  ..., -0.0346, -0.0326,  0.0482],\n",
       "         [ 0.0000,  0.2000,  0.3119,  ..., -0.0239,  0.0638,  0.0216],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " tensor([285., 321., 276., 374., 350., 327., 381., 270., 253., 250., 348., 352.,\n",
       "         366., 282., 259., 361., 338., 393., 300., 306., 299., 367., 377., 375.,\n",
       "         386., 268., 347., 385., 255., 307., 277., 302., 356., 398., 320., 333.,\n",
       "         369., 266., 260., 334., 378., 252., 353., 371., 324., 319., 291., 345.,\n",
       "         363., 311., 323., 387., 359., 248., 368., 290., 365., 329., 325., 287.,\n",
       "         336., 391., 295., 314., 309., 343., 357., 358., 258., 389., 349., 318.,\n",
       "         310., 272., 263., 388., 380., 289., 395., 376., 354., 316., 247., 390.,\n",
       "         280., 261., 273., 335., 342., 288., 346., 245., 328., 246., 308., 267.,\n",
       "         293., 351., 331., 294., 344., 296., 251., 332., 312., 360., 249., 330.,\n",
       "         326., 265., 355., 364., 274., 256., 397., 337., 372., 396., 370., 278.,\n",
       "         283., 262., 297., 254., 313., 298., 305., 257., 244., 373., 317., 275.,\n",
       "         379., 292., 279.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.]),\n",
       " tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "         126, 127, 128, 129, 130, 131, 132, 133, 134,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T17:11:33.628791Z",
     "start_time": "2025-03-29T17:11:33.626085Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d004e1fb6f3fd261",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T17:11:33.753309Z",
     "start_time": "2025-03-29T17:11:33.751562Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f30f1173b999e635",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
